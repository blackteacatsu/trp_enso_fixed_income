{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee50d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 32 CPU cores\n",
      "\n",
      "Dask cluster ready for parallel processing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd # type: ignore\n",
    "import xarray as xr # type: ignore\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "import scipy\n",
    "\n",
    "# Get number of CPU cores\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "print(f\"Detected {n_cores} CPU cores\")\n",
    "\n",
    "# put this on your fastest SSD/NVMe with plenty of free space\n",
    "SPILL_DIR = r\"C:\\dask-spill\"\n",
    "os.makedirs(SPILL_DIR, exist_ok=True)\n",
    "\n",
    "dask.config.set({\n",
    "    \"distributed.worker.local-directory\": SPILL_DIR,\n",
    "    \"distributed.worker.memory.target\": 0.70,    # start spilling earlier\n",
    "    \"distributed.worker.memory.spill\": 0.80,\n",
    "    \"distributed.worker.memory.pause\": 0.88,\n",
    "    \"distributed.worker.memory.terminate\": 0.98,\n",
    "    \"distributed.comm.compression\": \"auto\",      # or None if codec issues reappear\n",
    "})\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=6,\n",
    "    threads_per_worker=4,       # 14900K has plenty; adjust if you want\n",
    "    processes=False,             # threads-only on Windows\n",
    "    memory_limit=\"auto\",         # leave headroom for OS/Jupyter\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"\\nDask cluster ready for parallel processing!\")\n",
    "\n",
    "# HDF5 locking issues (Rockfish/HPC): set once per process if needed\n",
    "# os.environ.setdefault(\"HDF5_USE_FILE_LOCKING\", \"FALSE\")\n",
    "# os.environ[\"NETCDF_HDF5_FILE_LOCKING\"] = \"FALSE\"\n",
    "#xr.set_options(display_style=\"text\") # Display xarray in plain text\n",
    "\n",
    "# --path--\n",
    "# era5land_quarterly_data = \"/vast/bzaitch1/trp_climate_model_data/era5land_1970_2024_qtrmean\" # Location on Rockfish\n",
    "# era5land_quarterly_data_directory = Path(\"/Users/kris/Local Job Backup/test set/\") # Location on local machine\n",
    "#output_directory  = \"/vast/bzaitch1/trp_climate_model_data/out\"\n",
    "#os.makedirs(output_directory, exist_ok=True)\n",
    "QUARTER_DIR = Path(rf\"E:\\backup\\trp_climate_model_data\\era5land_1970_2024_qtrmean\")\n",
    "\n",
    "# Hourly filename pattern:\n",
    "# One file per month:    YYYY_MM.nc\n",
    "PATTERN_PER_MONTH = \"{y}_{m}.nc\"   # for one file per month\n",
    "\n",
    "# cvar_aliases = {\n",
    "#     'tp' : 'total precipitation', \n",
    "#     'e' : 'evaporation',\n",
    "# }\n",
    "\n",
    "evar_aliases = {\n",
    "    'y' : 'Log of real GDP', \n",
    "    'Dp' : 'Inflation rate (Based on CPI)',\n",
    "    'eq' : 'Log of real equity prices',\n",
    "    'ep' : 'Log of real exchange rate',\n",
    "    'r' : 'Short-term interest rate',\n",
    "    'lr' : 'Long-term interest rate',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9qxg4du2bnh",
   "metadata": {},
   "source": [
    "## Load Climate Data for Drought Analysis\n",
    "\n",
    "We'll use precipitation and evaporation data to calculate drought indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965fd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fix longitude from 0..360 to -180..180 and sort ---\n",
    "def fix_long(da):\n",
    "    if float(da.longitude.max()) > 180:\n",
    "        lon_new = ((da.longitude + 180) % 360) - 180\n",
    "        da = da.assign_coords(longitude=lon_new).sortby(\"longitude\")\n",
    "    \n",
    "    return da\n",
    "\n",
    "# --- make latitude ascending (optional but helpful for weights) ---\n",
    "def sort_lat(da):\n",
    "    if da.latitude[0] > da.latitude[-1]:\n",
    "        da = da.sortby(\"latitude\")\n",
    "    \n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auygu92p1vt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded climate data:\n",
      "  Precipitation shape: (160, 1801, 3600)\n",
      "  Evaporation shape: (160, 1801, 3600)\n",
      "  Time range: 1980-03-31T00:00:00.000000000 to 2019-12-31T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "# Load quarterly climate data\n",
    "test_data_set = QUARTER_DIR / '*.nc'\n",
    "ds = xr.open_mfdataset(str(test_data_set), engine='netcdf4')\n",
    "\n",
    "# Extract precipitation and evaporation\n",
    "da_precip = ds['tp']  # Total precipitation\n",
    "da_evap = ds['e']     # Evaporation\n",
    "\n",
    "# da_precip = fix_long(da_precip)\n",
    "# da_evap = fix_long(da_evap) \n",
    "\n",
    "print(\"Loaded climate data:\")\n",
    "print(f\"  Precipitation shape: {da_precip.shape}\")\n",
    "print(f\"  Evaporation shape: {da_evap.shape}\")\n",
    "print(f\"  Time range: {da_precip.valid_time.values[0]} to {da_precip.valid_time.values[-1]}\")\n",
    "\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8hg6znmxywf",
   "metadata": {},
   "source": [
    "## Drought Detection Functions\n",
    "\n",
    "We'll implement multiple drought indices:\n",
    "1. **Precipitation Anomaly** - Detrended precipitation deviations from normal\n",
    "2. **Water Balance Anomaly** - Precipitation minus evaporation anomalies\n",
    "3. **Drought Intensity** - Normalized drought severity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "r2of508g238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drought detection functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def detrend_dim(da, dim, deg=1):\n",
    "    \"\"\"Remove linear trend from data array.\"\"\"\n",
    "    p = da.polyfit(dim=dim, deg=deg)\n",
    "    fit = xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    return da - fit\n",
    "\n",
    "def calculate_drought_anomaly(da, var_name='precipitation'):\n",
    "    \"\"\"\n",
    "    Calculate drought anomaly from climate variable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    da : xr.DataArray\n",
    "        Climate variable (e.g., precipitation, evaporation)\n",
    "    var_name : str\n",
    "        Name of the variable for metadata\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xr.DataArray : Standardized anomalies (negative = drought conditions)\n",
    "    \"\"\"\n",
    "    print(f\"Calculating drought anomaly for {var_name}...\")\n",
    "    \n",
    "    # Save metadata\n",
    "    original_attrs = da.attrs.copy()\n",
    "    \n",
    "    # Calculate seasonal climatology (by quarter)\n",
    "    climatology = da.groupby(\"valid_time.quarter\").mean(dim='valid_time')\n",
    "    \n",
    "    # Remove seasonal cycle to get anomalies\n",
    "    anomalies = da.groupby(\"valid_time.quarter\") - climatology\n",
    "    \n",
    "    # Detrend the anomalies\n",
    "    detrended = detrend_dim(anomalies, 'valid_time')\n",
    "    \n",
    "    # Standardize (convert to z-scores)\n",
    "    # Calculate standard deviation by quarter\n",
    "    std = da.groupby(\"valid_time.quarter\").std(dim='valid_time')\n",
    "    standardized = detrended.groupby(\"valid_time.quarter\") / std\n",
    "    \n",
    "    # Restore metadata\n",
    "    standardized.attrs = original_attrs\n",
    "    standardized.attrs['long_name'] = f'{var_name} drought anomaly (standardized)'\n",
    "    standardized.attrs['interpretation'] = 'Negative values indicate drought conditions'\n",
    "    standardized.name = f'{var_name}_drought_anomaly'\n",
    "    \n",
    "    print(f\"  ✓ Calculated standardized anomalies\")\n",
    "    return standardized\n",
    "\n",
    "def calculate_water_balance_drought(precip, evap):\n",
    "    \"\"\"\n",
    "    Calculate drought index based on water balance (P - E).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    precip : xr.DataArray\n",
    "        Precipitation data\n",
    "    evap : xr.DataArray\n",
    "        Evaporation data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xr.DataArray : Water balance drought index (negative = drought)\n",
    "    \"\"\"\n",
    "    print(\"Calculating water balance drought index...\")\n",
    "    \n",
    "    # Convert evaporation to positive values (it's stored as negative)\n",
    "    #evap_positive = -evap if evap.mean() < 0 and print('Convert evaporation to positive values (if it is stored as negative)') else evap\n",
    "    \n",
    "    # Calculate water balance\n",
    "    water_balance = precip - evap\n",
    "    \n",
    "    # Calculate anomaly\n",
    "    climatology = water_balance.groupby(\"valid_time.quarter\").mean(dim='valid_time')\n",
    "    anomalies = water_balance.groupby(\"valid_time.quarter\") - climatology\n",
    "    \n",
    "    # Detrend\n",
    "    detrended = detrend_dim(anomalies, 'valid_time')\n",
    "    \n",
    "    # Standardize\n",
    "    std = water_balance.groupby(\"valid_time.quarter\").std(dim='valid_time')\n",
    "    standardized = detrended.groupby(\"valid_time.quarter\") / std\n",
    "    \n",
    "    # Set metadata\n",
    "    standardized.attrs['long_name'] = 'Water balance drought index (P-E)'\n",
    "    standardized.attrs['interpretation'] = 'Negative values indicate drought (water deficit)'\n",
    "    standardized.name = 'water_balance_drought'\n",
    "    \n",
    "    print(\"  ✓ Calculated water balance drought index\")\n",
    "    return standardized\n",
    "\n",
    "def classify_drought_severity(drought_index):\n",
    "    \"\"\"\n",
    "    Classify drought severity based on standardized index.\n",
    "    \n",
    "    Classification:\n",
    "    - No drought: > -0.5\n",
    "    - Mild drought: -0.5 to -1.0\n",
    "    - Moderate drought: -1.0 to -1.5\n",
    "    - Severe drought: -1.5 to -2.0\n",
    "    - Extreme drought: < -2.0\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    drought_index : xr.DataArray\n",
    "        Standardized drought index\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xr.DataArray : Drought severity classification (0-4)\n",
    "    \"\"\"\n",
    "    severity = xr.zeros_like(drought_index)\n",
    "    severity = xr.where(drought_index <= -0.5, 1, severity)  # Mild\n",
    "    severity = xr.where(drought_index <= -1.0, 2, severity)  # Moderate\n",
    "    severity = xr.where(drought_index <= -1.5, 3, severity)  # Severe\n",
    "    severity = xr.where(drought_index <= -2.0, 4, severity)  # Extreme\n",
    "    \n",
    "    severity.attrs['long_name'] = 'Drought severity classification'\n",
    "    severity.attrs['classes'] = '0=None, 1=Mild, 2=Moderate, 3=Severe, 4=Extreme'\n",
    "    severity.name = 'drought_severity'\n",
    "    \n",
    "    return severity\n",
    "\n",
    "print(\"Drought detection functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crxmbz1saxn",
   "metadata": {},
   "source": [
    "## Calculate Drought Indices\n",
    "\n",
    "Compute multiple drought indicators for correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "zmnqymmifke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CALCULATING DROUGHT INDICES\n",
      "============================================================\n",
      "Calculating drought anomaly for precipitation...\n",
      "  ✓ Calculated standardized anomalies\n",
      "Calculating water balance drought index...\n",
      "  ✓ Calculated water balance drought index\n",
      "\n",
      "============================================================\n",
      "DROUGHT INDICES CALCULATED\n",
      "============================================================\n",
      "Precipitation drought: (160, 1801, 3600)\n",
      "Water balance drought: (160, 1801, 3600)\n"
     ]
    }
   ],
   "source": [
    "# Calculate drought indices\n",
    "print(\"=\"*60)\n",
    "print(\"CALCULATING DROUGHT INDICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Precipitation drought anomaly\n",
    "drought_precip = calculate_drought_anomaly(da_precip, var_name='precipitation')\n",
    "\n",
    "# 2. Water balance drought (P - E)\n",
    "drought_water_balance = calculate_water_balance_drought(da_precip, da_evap)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DROUGHT INDICES CALCULATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Precipitation drought: {drought_precip.shape}\")\n",
    "print(f\"Water balance drought: {drought_water_balance.shape}\")\n",
    "\n",
    "# Store in a dictionary for easy access\n",
    "drought_indices = {\n",
    "    #'precipitation': drought_precip,\n",
    "    'water_balance': drought_water_balance\n",
    "}\n",
    "\n",
    "drought_index_names = {\n",
    "    'precipitation': 'Precipitation Deficit Index',\n",
    "    'water_balance': 'Water Balance Deficit Index (P-E)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44023e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 19:48:46,876 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.31 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:22,897 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.05 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:23,170 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.05 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:23,498 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.09 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:23,764 - distributed.worker.memory - WARNING - Worker is at 90% memory usage. Pausing worker.  Process memory: 7.20 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:24,088 - distributed.worker.memory - WARNING - Worker is at 90% memory usage. Pausing worker.  Process memory: 7.20 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:24,361 - distributed.worker.memory - WARNING - Worker is at 90% memory usage. Pausing worker.  Process memory: 7.20 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:24,438 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.23 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:24,438 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.23 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:24,438 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.23 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:24,438 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.23 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:24,438 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.23 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:50:24,445 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.23 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:40,804 - distributed.worker.memory - WARNING - Worker is at 98% memory usage. Pausing worker.  Process memory: 7.86 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:41,097 - distributed.worker.memory - WARNING - Worker is at 97% memory usage. Pausing worker.  Process memory: 7.77 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:41,399 - distributed.worker.memory - WARNING - Worker is at 97% memory usage. Pausing worker.  Process memory: 7.79 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:41,685 - distributed.worker.memory - WARNING - Worker is at 97% memory usage. Pausing worker.  Process memory: 7.80 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:41,976 - distributed.worker.memory - WARNING - Worker is at 100% memory usage. Pausing worker.  Process memory: 7.99 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:42,257 - distributed.worker.memory - WARNING - Worker is at 100% memory usage. Pausing worker.  Process memory: 7.99 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:42,718 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:42,719 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:42,719 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:42,720 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:42,720 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:42,721 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,001 - distributed.worker.memory - WARNING - Worker is at 95% memory usage. Pausing worker.  Process memory: 7.58 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,002 - distributed.worker.memory - WARNING - Worker is at 95% memory usage. Pausing worker.  Process memory: 7.58 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,002 - distributed.worker.memory - WARNING - Worker is at 95% memory usage. Pausing worker.  Process memory: 7.58 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,002 - distributed.worker.memory - WARNING - Worker is at 95% memory usage. Pausing worker.  Process memory: 7.58 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,003 - distributed.worker.memory - WARNING - Worker is at 95% memory usage. Pausing worker.  Process memory: 7.58 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,003 - distributed.worker.memory - WARNING - Worker is at 95% memory usage. Pausing worker.  Process memory: 7.58 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,221 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,222 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,222 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,224 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,224 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:43,225 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.88 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,117 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 7.11 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,117 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 7.11 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,118 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 7.11 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,118 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 7.11 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,118 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 7.11 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,119 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 7.11 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,318 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 6.74 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,318 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 6.74 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,319 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 6.74 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,320 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 6.74 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,320 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 6.74 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,320 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 6.74 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,701 - distributed.worker.memory - WARNING - Worker is at 95% memory usage. Pausing worker.  Process memory: 7.61 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,837 - distributed.worker.memory - WARNING - Worker is at 97% memory usage. Pausing worker.  Process memory: 7.75 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,839 - distributed.worker.memory - WARNING - Worker is at 96% memory usage. Pausing worker.  Process memory: 7.66 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,839 - distributed.worker.memory - WARNING - Worker is at 96% memory usage. Pausing worker.  Process memory: 7.66 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,839 - distributed.worker.memory - WARNING - Worker is at 96% memory usage. Pausing worker.  Process memory: 7.67 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:46,839 - distributed.worker.memory - WARNING - Worker is at 96% memory usage. Pausing worker.  Process memory: 7.67 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:47,108 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.91 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:47,109 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.91 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:47,109 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.91 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:47,110 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.91 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:47,111 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.91 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:51:47,111 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Resuming worker. Process memory: 6.91 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:52:31,515 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.02 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:52:31,806 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.02 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:52:32,093 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.02 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:52:32,386 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.02 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:52:32,682 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.02 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:52:32,972 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 7.02 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:53:46,936 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.51 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 19:58:46,934 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 8.82 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:03:47,036 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 9.59 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:08:47,091 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 9.90 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:13:47,119 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 9.73 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:18:47,134 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 9.73 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:23:47,205 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 9.73 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:28:47,224 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 9.73 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:33:47,337 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:38:47,442 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:43:47,446 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:48:47,526 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:53:47,615 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 20:58:47,624 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:03:47,624 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:08:47,715 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:13:47,721 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:18:47,728 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:23:47,731 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:28:47,818 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:33:47,902 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:38:47,929 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:43:47,954 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:48:48,028 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:53:48,164 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 21:58:48,216 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:03:48,318 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:08:48,640 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:13:48,727 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:18:48,811 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:23:48,821 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:28:48,816 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:33:48,822 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:38:48,899 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:43:48,900 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:48:48,965 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:53:49,008 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n",
      "2025-11-03 22:58:49,023 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.18 GiB -- Worker memory limit: 7.97 GiB\n"
     ]
    }
   ],
   "source": [
    "# Quick check: plot detrended anomalies (spatial average time series)\n",
    "# Use subset for faster plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Downsample spatially for quick check (every 20th point)\n",
    "det_da_sample = drought_water_balance.isel(latitude=slice(None, None, 20), longitude=slice(None, None, 20))\n",
    "\n",
    "evar_label = drought_index_names.get('water_balance')\n",
    "\n",
    "# Compute and plot\n",
    "det_da_sample.mean(dim=['latitude','longitude']).plot(figsize=(12, 4))\n",
    "plt.title(f'Detrended {evar_label} Anomalies (Global Average - Downsampled)')\n",
    "plt.xlabel(f'Time') #(°C)\n",
    "plt.ylabel(f'{evar_label} Anomaly (m)') #(°C)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nqa7bec0k8g",
   "metadata": {},
   "source": [
    "## Correlation with Economic Data\n",
    "\n",
    "Correlate drought indices with economic indicators for all countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3l6kze20lu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drought correlation function loaded!\n"
     ]
    }
   ],
   "source": [
    "def correlate_drought_with_econ(drought_index, econ_var_series):\n",
    "    \"\"\"\n",
    "    Correlate drought index at every grid point with an economic time series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    drought_index : xr.DataArray\n",
    "        Drought index with dimensions (valid_time, latitude, longitude)\n",
    "    econ_var_series : pd.Series\n",
    "        Economic indicator time series with datetime index\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xr.DataArray : Correlation coefficients at each grid point\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find overlapping time period\n",
    "    climate_times = pd.DatetimeIndex(drought_index.valid_time.values)\n",
    "    econ_times = econ_var_series.index\n",
    "    \n",
    "    # Get intersection of times\n",
    "    common_times = climate_times.intersection(econ_times)\n",
    "    \n",
    "    if len(common_times) == 0:\n",
    "        raise ValueError(\"No overlapping time periods!\")\n",
    "    \n",
    "    print(f\"    Found {len(common_times)} overlapping time periods\")\n",
    "    \n",
    "    # Subset both datasets to common times\n",
    "    drought_subset = drought_index.sel(valid_time=common_times)\n",
    "    econ_subset = econ_var_series.loc[common_times]\n",
    "    \n",
    "    # Remove any NaN values from economic series\n",
    "    valid_mask = econ_subset.notna()\n",
    "    if not valid_mask.all():\n",
    "        print(f\"    Removing {(~valid_mask).sum()} NaN values from economic series\")\n",
    "        drought_subset = drought_subset.sel(valid_time=valid_mask.values)\n",
    "        econ_subset = econ_subset[valid_mask]\n",
    "    \n",
    "    # Convert to xarray DataArray\n",
    "    econ_da = xr.DataArray(\n",
    "        econ_subset.values,\n",
    "        coords={'valid_time': drought_subset.valid_time},\n",
    "        dims=['valid_time']\n",
    "    )\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation_map = xr.corr(drought_subset, econ_da, dim='valid_time')\n",
    "    \n",
    "    return correlation_map\n",
    "\n",
    "print(\"Drought correlation function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbom07pu3o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all countries and economic variables for ALL drought indices\n",
    "df = pd.ExcelFile(\"./static/df_country_data_climate.xlsx\")\n",
    "list_countries = df.sheet_names\n",
    "\n",
    "# Dictionary structure: {drought_type: {country: {econ_var: correlation_map}}}\n",
    "all_drought_correlations = {}\n",
    "\n",
    "print(f\"Processing drought correlations for {len(list_countries)} countries...\")\n",
    "print(f\"Drought indices: {list(drought_indices.keys())}\")\n",
    "print(f\"Economic variables: {list(evar_aliases.keys())}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Process each drought index type\n",
    "for drought_type, drought_data in drought_indices.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING: {drought_index_names[drought_type]}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_drought_correlations[drought_type] = {}\n",
    "    \n",
    "    for country in tqdm(list_countries, desc=f\"Countries ({drought_type})\"):\n",
    "        print(f\"\\n  {country}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load economic data\n",
    "            econ_data = pd.read_excel(df, sheet_name=country)\n",
    "            econ_data = econ_data[3:].copy()\n",
    "            econ_data['time'] = pd.to_datetime(econ_data['Unnamed: 0'])\n",
    "            econ_data = econ_data.set_index('time')\n",
    "            econ_data = econ_data.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "            \n",
    "            # Initialize correlation maps for this country\n",
    "            correlation_maps = {}\n",
    "            \n",
    "            # Loop through economic variables\n",
    "            for econ_var in evar_aliases.keys():\n",
    "                if econ_var not in econ_data.columns:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    econ_series = econ_data[econ_var]\n",
    "                    \n",
    "                    # Check data quality\n",
    "                    nan_ratio = econ_series.isna().sum() / len(econ_series)\n",
    "                    if nan_ratio > 0.5:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute correlation map\n",
    "                    print(f\"    Computing: {evar_aliases[econ_var]}...\")\n",
    "                    corr_map = correlate_drought_with_econ(drought_data, econ_series)\n",
    "                    correlation_maps[econ_var] = corr_map\n",
    "                    print(f\"      ✓ Complete\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      ✗ Error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Store results\n",
    "            all_drought_correlations[drought_type][country] = correlation_maps\n",
    "            \n",
    "            # Save individual country results\n",
    "            out_path = Path(r'.\\cache\\tmp\\drought') / f\"{drought_type}_{country}.pkl\"\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with out_path.open('wb') as f:\n",
    "                pickle.dump(correlation_maps, f)\n",
    "            \n",
    "            print(f\"    ✓ Saved {len(correlation_maps)} correlation maps\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Failed: {e}\")\n",
    "            continue\n",
    "\n",
    "# Save all results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for drought_type, country_data in all_drought_correlations.items():\n",
    "    print(f\"\\n{drought_index_names[drought_type]}:\")\n",
    "    print(f\"  Processed {len(country_data)} countries\")\n",
    "    total_vars = sum(len(maps) for maps in country_data.values())\n",
    "    print(f\"  Total correlations: {total_vars}\")\n",
    "\n",
    "# Save complete dataset\n",
    "out_path = Path(r'.\\cache\\tmp\\drought') / \"all_drought_correlations.pkl\"\n",
    "with out_path.open('wb') as f:\n",
    "    pickle.dump(all_drought_correlations, f)\n",
    "print(f\"\\n✓ Saved all drought correlations to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0vu1hp294rdi",
   "metadata": {},
   "source": [
    "## Visualization: Drought-GDP Correlations\n",
    "\n",
    "Generate plots for all countries showing drought impact on GDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "st1l3w9e8ej",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PLOTTING: Water Balance Deficit Index (P-E) - GDP Correlations\n",
      "============================================================\n",
      "Saving to: cache\\drought_plots\\water_balance\\gdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "water_balance:   0%|          | 0/33 [26:40<?, ?it/s]\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-33754460' coro=<Client._gather.<locals>.wait() done, defined at c:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\site-packages\\distributed\\client.py:2384> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\site-packages\\distributed\\client.py\", line 2393, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compute if dask array\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(corr_map_lazy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     corr_map \u001b[38;5;241m=\u001b[39m corr_map_lazy\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     corr_map \u001b[38;5;241m=\u001b[39m corr_map_lazy\n",
      "File \u001b[1;32mc:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xarray\\core\\dataarray.py:1102\u001b[0m, in \u001b[0;36mDataArray.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;124;03mremote source into memory and return a new array. The original is\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;124;03mleft unaltered.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xarray\\core\\dataarray.py:1076\u001b[0m, in \u001b[0;36mDataArray.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m: T_DataArray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_DataArray:\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1076\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_temp_dataset()\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1077\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[1;32mc:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xarray\\core\\dataset.py:792\u001b[0m, in \u001b[0;36mDataset.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    789\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    791\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[1;32m--> 792\u001b[0m evaluated_data \u001b[38;5;241m=\u001b[39m chunkmanager\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data):\n\u001b[0;32m    795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xarray\\core\\daskmanager.py:70\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[1;34m(self, *data, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: DaskArray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compute(\u001b[38;5;241m*\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[0;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[1;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(expr, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "File \u001b[1;32mc:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Kris\\AppData\\Local\\anaconda3\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate GDP correlation plots for all drought indices and countries\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "gdp_var = 'y'  # Real GDP variable\n",
    "\n",
    "for drought_type, country_data in all_drought_correlations.items():\n",
    "    # Create plots directory for this drought type\n",
    "    plots_dir = Path(r'.\\cache\\drought_plots') / drought_type / 'gdp'\n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PLOTTING: {drought_index_names[drought_type]} - GDP Correlations\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Saving to: {plots_dir}\")\n",
    "    \n",
    "    successful = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for country, maps in tqdm(country_data.items(), desc=f\"{drought_type}\"):\n",
    "        if gdp_var not in maps:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            corr_map_lazy = maps[gdp_var]\n",
    "            \n",
    "            # Compute if dask array\n",
    "            if hasattr(corr_map_lazy, 'compute'):\n",
    "                corr_map = corr_map_lazy.compute()\n",
    "            else:\n",
    "                corr_map = corr_map_lazy\n",
    "            \n",
    "            # Create the plot\n",
    "            fig, ax = plt.subplots(figsize=(14, 7))\n",
    "            im = corr_map.plot(ax=ax, cmap='RdBu_r', vmin=-0.8, vmax=0.8,\n",
    "                              cbar_kwargs={'label': 'Correlation Coefficient', 'shrink': 0.8})\n",
    "            \n",
    "            ax.set_title(f'{country} - {evar_aliases[gdp_var]} vs {drought_index_names[drought_type]}',\n",
    "                        fontsize=14, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel('Longitude', fontsize=11)\n",
    "            ax.set_ylabel('Latitude', fontsize=11)\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # Add statistics box\n",
    "            corr_values = corr_map.values\n",
    "            valid_corr = corr_values[~np.isnan(corr_values)]\n",
    "            if len(valid_corr) > 0:\n",
    "                stats_text = (f'Mean corr: {valid_corr.mean():.3f}\\n'\n",
    "                             f'Max corr: {valid_corr.max():.3f}\\n'\n",
    "                             f'Min corr: {valid_corr.min():.3f}\\n'\n",
    "                             f'Interpretation: Negative = drought reduces GDP')\n",
    "                ax.text(0.02, 0.98, stats_text,\n",
    "                       transform=ax.transAxes,\n",
    "                       fontsize=9,\n",
    "                       verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the plot\n",
    "            plot_filename = plots_dir / f\"{country}_GDP_vs_{drought_type}.png\"\n",
    "            plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error plotting {country}: {e}\")\n",
    "            plt.close('all')\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"  ✓ Successfully plotted: {successful} countries\")\n",
    "    print(f\"  ⚠ Skipped: {skipped} countries\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL DROUGHT-GDP PLOTS COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l0ppc4ngwe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a complete drought detection and economic correlation workflow:\n",
    "\n",
    "### Drought Indices Computed:\n",
    "1. **Precipitation Deficit Index**: Standardized precipitation anomalies (negative = drought)\n",
    "2. **Water Balance Deficit Index**: Precipitation minus Evaporation anomalies (negative = water deficit/drought)\n",
    "\n",
    "### Analysis Pipeline:\n",
    "1. Load ERA5-Land quarterly climate data (precipitation & evaporation)\n",
    "2. Calculate drought indices with seasonal detrending and standardization\n",
    "3. Correlate drought indices with economic variables for all countries\n",
    "4. Generate correlation maps and save to organized folders\n",
    "\n",
    "### Interpretation:\n",
    "- **Negative correlations**: Drought (water deficit) negatively impacts the economic variable\n",
    "- **Positive correlations**: Drought positively impacts the economic variable (unusual)\n",
    "- **Near-zero correlations**: Little to no relationship between drought and economy\n",
    "\n",
    "### Output Structure:\n",
    "```\n",
    "cache/\n",
    "├── drought/                           # Pickle files with correlation data\n",
    "│   ├── precipitation_Argentina.pkl\n",
    "│   ├── water_balance_Argentina.pkl\n",
    "│   └── all_drought_correlations.pkl\n",
    "└── drought_plots/                     # PNG plots\n",
    "    ├── precipitation/\n",
    "    │   └── gdp/\n",
    "    │       ├── Argentina_GDP_vs_precipitation.png\n",
    "    │       └── ...\n",
    "    └── water_balance/\n",
    "        └── gdp/\n",
    "            ├── Argentina_GDP_vs_water_balance.png\n",
    "            └── ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
